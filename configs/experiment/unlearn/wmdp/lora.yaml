# @package _global_

defaults:
  - override /model: Qwen2.5-3B-Instruct-lora
  - override /trainer: RMU
  - override /data: unlearn
  - override /data/datasets@data.forget: WMDP_forget
  - override /data/datasets@data.retain: WMDP_retain
  - override /eval: lm_eval

data_split: cyber

data:
  anchor: forget
  forget:
    WMDP_forget: 
      args:
        hf_args:
          data_files: data/wmdp/wmdp-corpora/${data_split}-forget-corpus.jsonl
  retain:
    WMDP_retain:
      args:
        hf_args:
          data_files: data/wmdp/wmdp-corpora/${data_split}-retain-corpus.jsonl

eval:
  lm_eval:
    tasks:
      - wmdp_${data_split}
      - mmlu

collator:
  DataCollatorForSupervisedDataset:
    args:
      padding_side: left # Usually left but for mistral and zephyr its right (https://github.com/hongshi97/CAD/issues/2)

trainer:
  args:
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 16
    learning_rate: 1e-4  # Higher learning rate for LoRA
    eval_strategy: steps
    eval_steps: 0.5
    max_steps: 80
    lr_scheduler_type: constant
    warmup_epochs: 0.1  # Shorter warmup for LoRA
    logging_steps: 10
    save_steps: 500
    save_strategy: steps
    load_best_model_at_end: false  # Disable to avoid metric issues
    save_total_limit: 2
    remove_unused_columns: false
    dataloader_pin_memory: false
    seed: 42

  method_args:
    # The params here are more dependent on model and dataset. Tune them carefully to work
    # NOTE: module_regex depends on model architecture and PEFT wrapper:
    # - For Qwen with PEFT LoRA: use "base_model.model.layers.7" (PEFT wraps base_model)
    # - For Qwen with custom LoRA wrapper: try "model.model.layers.7"
    # - For Zephyr/Mistral without LoRA: use "model.layers.7"
    # - Adjust layer number based on model size (e.g., 7 for 7B models, 3-4 for smaller models)
    # If you get "No module matched" error, the improved error message will show available modules
    gamma: 1.0
    steering_coeff: 2
    retain_loss_type: EMBED_DIFF
    alpha: 1
    module_regex: base_model\.model\.layers\.7  # Default for Qwen with PEFT LoRA
    trainable_params_regex: 
      - .*  # Update all parameters (default for LoRA)
      # - base_model\.model\.layers\.(5|6|7)\.mlp\.down_proj\.weight # Alternative: update only specific weights

task_name: wmdp_unlearn_lora
